Gradient descent

a = a - lr * delJ/dela
Always update terms simultaneously
Correct:
temp0 = a - lr*delJ/dela
temp1 = b - lr*delJ/delb
a = temp0
b = temp1

Incorrect:
temp0 = a - lr*delJ/dela
a = temp0
temp1 = b - lr*delJ.delb
b = temp1

The cost function for linear regression will always be a convex function. thus gradient descent will always converge to the local optimum
Batch gradient descent: each step uses all the training examples
Normal equations method: A method from linear algebra that directly finds the optimum, without needing to iteratively converge.
But, it was found that gradient descent was better for large datasets.

Wrong answer:
Linear regression with one variable:
Suppose that for some linear regression problem (say, predicting housing prices as in the lecture), we have some training set, and for our training set we managed to find some (theta)0, (theta)1 such that J(theta0, theta1) = 0. Which of the statements below must be true? (Multiple correct)