GRADIENT DESCENT

Time complexity: O(kn^2)
a = a - lr * delJ/dela
Always update terms simultaneously
Correct:
temp0 = a - lr*delJ/dela
temp1 = b - lr*delJ/delb
a = temp0
b = temp1

Incorrect:
temp0 = a - lr*delJ/dela
a = temp0
temp1 = b - lr*delJ.delb
b = temp1

The cost function for linear regression will always be a convex function. thus gradient descent will always converge to the local optimum
Batch gradient descent: each step uses all the training examples
Normal equations method: A method from linear algebra that directly finds the optimum, without needing to iteratively converge.
But, it was found that gradient descent was better for large datasets.

Wrong answer:
Linear regression with one variable:
Suppose that for some linear regression problem (say, predicting housing prices as in the lecture), we have some training set, and for our training set we managed to find some (theta)0, (theta)1 such that J(theta0, theta1) = 0. Which of the statements below must be true? (Multiple correct)

LINEAR ALGEBRA

vector: an n x 1 matrix
1-indexed vector, 0-indexed vector

Matrix without an inverse is a singular or degenerate matrix

MULTIVARIATE LINEAR REGRESSION

x^(i) = input (features) of ith training example
Single variable hypotheses: h(x) = t0 + xt1

Multivariable hypotheses: h(x) = t0 + x,t, + x2t2 + x3t3 + x4t4
let x0 = 1 to facilitate vectorization of the hypothesis function

X = column_vector([x0, x1, x2, x3 ... xn])
t = column_vector([t0, t1, t2, t3 ... tn])

h(x) = innerProduct((theta_transpose), x)
h(x) = x(theta)^T
General gradient descent
theta0 := theta_featureNumber - lr*(1/m) SUM(h(x^(houseNumber) - y^(houseNumber)))x_featureNumber^(houseNumber)

We can speed up gradient descent by having each of our input values in roughly the same range.
Gradient descent can work more quickly if the features are normalized or scaled
If the ellipse is thin, the gradient descent can take time and oscillate or meander about before converging
A scaled ellipse will be optimized earlier. GD can converge much faster
Get every feature to a [-1, 1] range
[0, 3] is acceptable; values in the vicinity are acceptable

Apply normalization too all features except x0
x(i) -> x(i) - mu(i)
i.e: replace x with (x - mean)/total

More general rule: 
replace x1 with (x1 - mean)/s1
where s1 = range(max - min)
or s1 = stdDeviation
(Quizzes in the course use range, programming assignments use standard deviation)

Feature scaling doesn't have to be too exact for gradient descent to work reasonably well

EG: Suppose you are using a learning algorithm to estimate the price of houses in a city. You want one of your features xi to capture the age of the house. In your training set, all of your houses have an age between 30 and 50 years, with an average of 38 years. Which of the following would you use as features assuming you use feature scaling and normalization?
Ans: x(i) = (age_of_house - 38) / 20

LEARNING RATE

Debugging gradient descent: Plot J(theta) vs no. of iterations. J(theta) should decrease after every iteration.
Example automatic convergence test: Declare convergence if J(theta) decreases by less than 10e-3 in one iteration.
If J(theta) is increasing, learning rate is probably too big.
For sufficiently small learning rate, J(theta) should decrease on every iteration.
Try learning rates as 0.0001, (0.0003), 0.001, (0.003), 0.01, (0.03), 0.1, (0.3), 1

POLYNOMIAL REGRESSION

We can create new features by ourselves
h(x) = t0 + xt1 + (x^2)t2
But quadratic models don't make practical sense as they have either a maxima or a minima, which might not correspond to probable values in the dataset.
Cubics might be better.
To fit a cubic function to a linear model, we use three features, let f be a feature, we then use f, f^2 and f^3 as three independent features and set them in a linear model
h(x) = t0 + ft1 + f^2t2 + f^3t3.

If features are chosen like this, feature scaling becomes increasingly important
Eg: h(x) = t0 + ft1 + sqrt(f)t2

Suppose you want to predict a house's price as a function of its size. Your model is
h(x) = t0 + (size)t1 + sqrt(size)t2
Suppose size ranges from 1 to 1000 (feet^2). You will implement this by fitting a model
h(x) = t0 + t1x1 + t2x2
Finally, suppose you want to use feature scaling (without mean normalization)
Which of the following choices for x1 and x2 should you use? (Note: sqrt(1000) ~ 32)
Ans: x1 = size/1000, x2 = sqrt(size)/32

NORMAL EQUATION

A method to solve for theta analytically.
Let J(theta) = a(theta^2) + b(theta) + c
find minima (when theta belongs to real numbers)
When theta is a vector, we can use partial derivatives to implement a similar process
Construct a matrix X that contains all features
Construct a vector y that contains all 'labels'
The value of theta that minimizes the cost function will then be:
theta = (X^T * X)^-1 * X^T * y
theta = inverse(transpose(X)*X)*transpose(X)*y
Set A = X^T * X. Then find A^-1
Octave: pinv(X'*X)*X'*y
Imp: For this method, feature scaling isn't actually necessary
Advantages: Normal equation doesn't need to choose learning rate and doesn't need to iterate.
Disadvantages: Normal equation doesn't work well with large data. Slow if n is large.
Cost of inverting a matrix is approximately O(n^3)
Inverting a 100x100 matrix is fast enough.
1000x1000 is also good enough
At about n = 10000, consider switching to other methods or to gradient descent.
Normal equations don't work for logistic regression and other sophisticated learning algorithms.

Non invertibility: (happens very rarely) pinv command in octave will actually compute the value we want without throwing errors.
Common causes of non-invertibility:
1. Redundant features: Eg: x1 = size in feet^2, x2 = size in m^2
2. Too many features (eg: m <= n) may cause non-invertibility
	Delete some features or use regularization.

OCTAVE
Differences: 
2^6 can be used
!= corresponds to ~= in Octave
% writes a comment
xor(1, 0)
writing a semicolon after a variable declaration prevents printing of the value of the assigned variable
a = pi % assigns value 'pi' to a
printing can be done using the disp() command
disp(sprintf('2 decimals: %0.2f', a))
format long % causes numbers to be displayed to a lot more decimal places
format short % the default value
A = [1 2; 3 4; 5 6] % generates a matrix
v = 1:0.1:2 % equivalent to v = range(1, 2, 0.1), v will be a row vector and the upper limit is inclusive
ones(2, 3) % generates a 2 by 3 matrix with all ones
2*ones(2, 3) 5 generates a 2 by 3 matrix with all twos
eye(n) % generates n dimensional identity matrix
rand(1, 3) % generates a row vector with random floats (0 to 1) from uniform distribution
randn(1, 3) % generates a row vector with random floats from gaussian distribution
hist(w) % generates histogram for the data
hist(w, n) % generates a histogram with n bins for the data
help eye % brings help page for eye
size(A) % returns a 1 by 2 matrix with the dimensions of the matrix A
PS1('>> ') % changes default terminal carat from octave > to >>
size(A, 1) % returns the size of the first dimension
length(A) % returns the longer dimension. Applied usually to vectors
load featuresX.dat % loads the file
load('featuresX.dat') % does the same
who % shows what variables we have in the octave workspace or in the octave memory
whos % gives a detailed view
clear featuresX % clears memory
v = priceY(1:10) % returns first 10 elements
save hello.mat v; % saves the vector v to a file called hello.mat (in a binary format)
save hello.txt v -ascii; % saves the vector v to a file called hello.txt (in a human readable ASCII format)
clear % deletes all variables in the workspace
A(3, 2) % returns the element in the third row and second column of A
A(2,:) % returns all elements in the second row
A(:, 2) % returns all the elements in the second column
A([1 3], :) % returns all the elements in the first and third rows
A(:, 2) = [10; 11; 12] replaces the second column with the given new column vector
A = [A, [100, 101, 102]] % appends the given column vector to the right
A(:) % puts all elements of A into a single column vector (equivalent to np.ravel())
C = [A B] % horizontally concatenates the matrices A and B into another matrix C
C = [A;B] % vertically concatenates the matrix A and B into the new matrix C
A*B % multiplies two matrices A and B
A .* B % multiplies the elements of the two matrices elementwise
A .^ 2 % squares all the elements of the matrix A
1 ./ A % reciprocates all the elements of the matrix A
log(A) % elementwise logarithm of A
exp(A) % elementwise exponentiation of A
abs(A) % elementwise magnitude of A
A + ones(size(A)) % increments all the elements of A by one
A + 1 % does the exact same thing
A' % returns transpose of A
max(A) % returns the maximum value of vector A
[val, ind] = max(A) % returns the maximum value of the vector and its index
A < 3 % returns a boolean matrix after doing the required logical comparison
find(A < 3) % returns the indices of the elements that return true to the specified logical operation
magic(n) % returns a matrix of dimension n by n in which, all the diagonals, rows, columns sum up to the same thing
[r, c] = find(A >= 7) % returns the row numbers and column numbers of the matrices that satisfy the given condition separately
sum(A) % adds up a vector A and columnwise adds up a matrix A
prod(A) % multiplies the elements of a vector A and columnwise multiplies a matrix A
floor(A), ceil(A) % do what they should
max(rand(3), rand(3)) % returns a matrix with the max elements from the two random three dimensional matrices
max(A, [], 1) % returns columnwise maximum of the matrix. 1 tells it to return the max along the first dimension (the row dimension). A 2 will make it return the max along the second dimension (the column dimension). A 3 will return the entire matrix for a 2-dimensional matrix
max(max(A)) % returns the largest element of the matrix A
max(A(:)) % (max(np.ravel(A))) returns the same
sum(sum(A.*eye(9))) % sums up the major diagonal of the matrix
B = A.*eye(9); sum(B(:)) % does the same
sum(sum(A.*flipud(eye(9)))) % sums the minor diagonal of the matrix
% flipud stands for flip-up-down and flips the matrix
plot(x_axis, y_axis) % plots data
% plotting a new plot deletes the old plot.
hold on % preserves the plotted plot and plots the next plot on top of it.
plot(t, y1, 'r') % changes color to red
xlabel('time'); ylabel('value') % labels axes (equivalent to matplotlib)
legend('sin', 'cos'); % adds a legend (equivalent to matplotlib)
title('Interesting graph') % adds title
print -dpng 'plot.png' % saves plot
close % causes the open figure to go away
figure(1); plot(t, y1) % plots first plot in a new figure
figure(2); plot(t, y2) % specifying a different figure number opens a new plot
subplot(1, 2, 1); % subdivides the plot into sections. The first two parameters divide the figure into a 1x2 grid. The third argument 'n' accesses the nth figure
subplot(1, 2, 1); plot(t, y1);
subplot(1, 2, 2); plot(t, y2); % plots two plots side by side
axis([0.5 1 -1 1]) % sets the xrange and the yrange on the plot
clf; % clears a figure
A = magic(5); imagesc(A) % plots a matrix's values as hues
imagesc(A), colorbar, colormap gray; % plots a grayscale colormatrix mapped to the values of the matirx A itself and displays a colorbar beside it as reference
commands can be comma chained to print the results of multiple commands
commands can be semi-colon chained to print the results of multiple commands
Let v = zeros(10, 1);
for i = 1: 10,
	v(i) = 2^i;
	end;
	% v now stores 2^i for i = 1 to 10

indices = 1:10;
for i = indices,
disp(i);
end;
% does the exact same thing
% break and continue can be used as usual

i = 1;
while i <= 5,
v(i) = 100;
i = i + 1;
end;
% does the same thing using a while loop
if, elseif, else % equivalent to if, elif, else
Functions:
function y = squareThisnumber(x)
y = x^2
% save this in a .m file to be called by octave from the wroking directory
addpath('C:\path\to\directory') % adds the specified path to the octave functions searchpath
Octave supports multiple return statements % check file sqcu.m to see how to define such a function
% to obtain the return values of such a function, use [a, b] = function(x) or something like that

VECTORIZATION
h(x) = sum(thetai*xi);
h(x) = theta' * x;

Unvectorized implementation (MATLAB/Octave):
prediction = 0.0;
for j = 1:n+1,
prediction = prediction + theta(j) * x(j)
end;
% IMPORTANT: Matlab and Octave are 1-indexed. Therefore, theta0 ends up having an index of 1 and so-on

Vectorized implementation (MATLAB/Octave):
prediction = theta' * x;

Unvectorized implementation in C++
double prediction = 0.0;
for(int j = 0; j <=n; ++j) {
	prediction == theta[j] * x[j];
}

Vectorized implementation in C++:
// Using a library
double prediction = theta.transpose() * x;
// here * is a C++ overloaded operator

Vectorization optimizes algorithms

Q:
v = zeros(10, 1)
for i = 1:10,
	for j = 1:10,
		v(i) = v(i) + A(i, j) * x(j);
	end;
end;

To find product Ax, how would you vectorize this code without any for loops?
Hint: Dimension of the answer vector matters. 10x1 ~= 1x10

LOGISTIC REGRESSION

For classification problems.
Binary classification problems
Hypotheses representation:
We need 0 <= h(x) <= 1
h(x) = g(theta' * x)
where g is the sigmoid function or the logistic function
g(z) = 1 / (1 + e^-z)
g(z) outputs a value between 0 and 1
It has asymptotes at y = 1 and y = 0
Thus, h(x) must also be between 0 and 1
When h(x) outputs a number, we will treat that number as the estimated probability that y = 1 on input x
If h(x) = 0.7,
We will predict that there is a 70% chance of the output being 1
h(x) = P(y = 1|x ; theta) % read as 'probability that y = 1, given x, parameterized by theta'
y must be 0 or 1
P(y = 0|x ; theta) + P(y = 1|x ; theta) = 1
P(y = 0|x ; theta) = 1 - P(y = 1|x ; theta)
To normalize outputs, 
predict 'y = 1' if h(x) >= 0.5
predict 'y = 0' if h(x) < 0.5
g(z) >= 0.5 whenever z >= 0
h(x) = g(theta' * x)
h(x) >= 0.5 whenever (theta' * x) >= 0
h(x) < 0.5 whenever (theta' * x) < 0
Let h(x) = g(t0 + t1x1 + t2x2)
Let t = [-3;1;1]
Predict 'y = 1' if -3 + x1 + x2 >= 0, for some x1, x2 where x1 + x2 >= 3
Plotting x1 + x2 = 3 on a graph where the axes are x1 and x2, gives the line known as the decision boundary
x1 + x2 = 3 corresponds to h(x) = 0.5
We don't need to plot a training set in order to plot the decision boundary
Non-linear decision boundaries
We can add nonlinear features, eg x1, x2, x1^2, x2^2, etc to fit our model
Eg: if h(x) = g(t0 + t1x1 + t2x2 + t3x1^2 + t4x2^2)
if t0 = -1, t1 = 0, t2 = 0, t3 = 1, t4 = 1, we get a circle with radius 1
theta = [-1;0;0;1;1]
Therefore, predict 'y = 1' if -1 + x1^2 + x2^2 >= 0
Decision boundary is a property of the hypotheses, not the training set.
Once we have the parameter theta, that defines the decision boundary, not the training set
Using more complicated functions as the hypotheses, we can come up with complex-shaped decision boundaries
In retrospect:
h(x) >= 0.5 -> y = 1
h(x) < 0.5 -> y = 0
g(z) >= 0.5, when z >= 0
Thus, when theta' * x >= 0 => y = 1
when theta' * x < 0 => y = 0
Cost function:
m examples, x belongs to [x0, x1, x2...] where x0 is always 1. For every x, the label y is either 0 or 1
cost(h(x), y) = 1/2(h(x) - y)^2
In logistic regression, the cost function is non-convex
Using a squared cost function, the non-linear sigmoid term causes many local optima in the cost function. We would like te cost function to be convex. We might come up with a different cost function that is convex, so that we can apply gradient descent
We redefine the cost function as follows
Cost(h(x), y) = { -log(h(x)),  		if y = 1;
				{ -log(1 - h(x)), 	if y = 0;
J = 1/m(sum(Cost(i)))
This function works very well in this case as cost = 0 if y = 1, h(x) = 1
But as h(x) -> 0, Cost -> infinity
Captures the intuition that if h(x) = 0, (predict P(y = 1|x ; theta) = 0), but y = 1, we'll penalize the learning algorithm by a very large cost
For y = 0, the cost function looks flipped (a mirror image along the y axis) between the x values of 0 and 1.
Works on a similar intuition as explained above
Simplified cost function: 
Cost(h(x), y) = -ylog(h(x)) - (1 - y)log(1 - h(x))
Cost(h(x), y) = -log(1 - y + (-1)^(y+1)*h(x))
Thus, J = -1/m[sum(ylog(h(x)) + (1 - y)log(1 - h(x)))]
Vectorized: J(theta) = (1/m)*(-y'*log(h) - (1-y)'*log(1 - h))
this cost function is used, because it can be analysed in great detail by statistics using the principle of likelihood estimation. And this function is convex and used by most people for logistic regression.
To fit parameters, min J(theta) wrt theta
Gradient descent:
Repeat {
	thetaj = thetaj - learning_rate * sum((h(x) - y)*xj(i))
}
algorithm looks identical to linear regression. (h(x) is as different as can be)
Apply same methods to monitor gradient descent as in linear regression
Q: Vectorized implementation of gradient descent for logistic regression
Ans: theta = theta - learning_rate*(1/m)*sum[(h(x) - y) * x]
Vectorized implementation: theta = theta - (learning_rate/m)*X'*(g(X*theta) - y)
The idea of feature scaling applies to gradient descent for logistic regression too
To make logistic regression run quicker and scale much better to larger machine learning problems.
Optimization algorithms:
Gradient descent, Conjugate gradient, BFGS, L-BFGS; These have a clever inner loop called a line-search algorithm that automatically tries different learning rates
Advantages of algorithms other tan gradient descent:
no need to manually pick the learning rate alpha,
These are often faster than gradient descent
Disadvantages: more complex (do not implement these yourself)
Eg for costFunction
function [jVal, gradient] = costFunction(theta)
jVal = (theta(1) - 5)^2 + (theta(2) - 5)^2;
gradient = zeros(2, 1);
gradient(1) = 2*(theta(1) - 5);
gradient(2) = 2*(theta(2) - 5);
% Advanced optimization function fminunc() (function minimization unconstrained)
% options is a data structure that stores the options
% sets gradientObjective parameter to 'on'
options = optimset('GradObj', 'on', 'MaxIter', '100');
initialTheta = zeros(2, 1);
[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options)
% Gradient descent on steroids
% the exitFlag helps to verify whether or not the algorithm thinks it has converged
% parameter vector initialTheta has to be at least 2 dimensional, or fminunc will give funny results

Generalized costFunction:
function [jval, gradient] = costFunction(theta)
jVal = [code to compute J(theta)]
gradient(1) = [code to compute the partial derivative wrt theta0]
gradient(2) = [code to compute the partial derivative wrt theta1]
...
gradient(n+1) = [code to compute the partial derivative wrt thetan]