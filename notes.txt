GRADIENT DESCENT

Time complexity: O(kn^2)
a = a - lr * delJ/dela
Always update terms simultaneously
Correct:
temp0 = a - lr*delJ/dela
temp1 = b - lr*delJ/delb
a = temp0
b = temp1

Incorrect:
temp0 = a - lr*delJ/dela
a = temp0
temp1 = b - lr*delJ.delb
b = temp1

The cost function for linear regression will always be a convex function. thus gradient descent will always converge to the local optimum
Batch gradient descent: each step uses all the training examples
Normal equations method: A method from linear algebra that directly finds the optimum, without needing to iteratively converge.
But, it was found that gradient descent was better for large datasets.

Wrong answer:
Linear regression with one variable:
Suppose that for some linear regression problem (say, predicting housing prices as in the lecture), we have some training set, and for our training set we managed to find some (theta)0, (theta)1 such that J(theta0, theta1) = 0. Which of the statements below must be true? (Multiple correct)

LINEAR ALGEBRA

vector: an n x 1 matrix
1-indexed vector, 0-indexed vector

Matrix without an inverse is a singular or degenerate matrix

MULTIVARIATE LINEAR REGRESSION

x^(i) = input (features) of ith training example
Single variable hypotheses: h(x) = t0 + xt1

Multivariable hypotheses: h(x) = t0 + x,t, + x2t2 + x3t3 + x4t4
let x0 = 1 to facilitate vectorization of the hypothesis function

X = column_vector([x0, x1, x2, x3 ... xn])
t = column_vector([t0, t1, t2, t3 ... tn])

h(x) = innerProduct((theta_transpose), x)
h(x) = x(theta)^T
General gradient descent
theta0 := theta_featureNumber - lr*(1/m) SUM(h(x^(houseNumber) - y^(houseNumber)))x_featureNumber^(houseNumber)

We can speed up gradient descent by having each of our input values in roughly the same range.
Gradient descent can work more quickly if the features are normalized or scaled
If the ellipse is thin, the gradient descent can take time and oscillate or meander about before converging
A scaled ellipse will be optimized earlier. GD can converge much faster
Get every feature to a [-1, 1] range
[0, 3] is acceptable; values in the vicinity are acceptable

Apply normalization too all features except x0
x(i) -> x(i) - mu(i)
i.e: replace x with (x - mean)/total

More general rule: 
replace x1 with (x1 - mean)/s1
where s1 = range(max - min)
or s1 = stdDeviation
(Quizzes in the course use range, programming assignments use standard deviation)

Feature scaling doesn't have to be too exact for gradient descent to work reasonably well

EG: Suppose you are using a learning algorithm to estimate the price of houses in a city. You want one of your features xi to capture the age of the house. In your training set, all of your houses have an age between 30 and 50 years, with an average of 38 years. Which of the following would you use as features assuming you use feature scaling and normalization?
Ans: x(i) = (age_of_house - 38) / 20

LEARNING RATE

Debugging gradient descent: Plot J(theta) vs no. of iterations. J(theta) should decrease after every iteration.
Example automatic convergence test: Declare convergence if J(theta) decreases by less than 10e-3 in one iteration.
If J(theta) is increasing, learning rate is probably too big.
For sufficiently small learning rate, J(theta) should decrease on every iteration.
Try learning rates as 0.0001, (0.0003), 0.001, (0.003), 0.01, (0.03), 0.1, (0.3), 1

POLYNOMIAL REGRESSION

We can create new features by ourselves
h(x) = t0 + xt1 + (x^2)t2
But quadratic models don't make practical sense as they have either a maxima or a minima, which might not correspond to probable values in the dataset.
Cubics might be better.
To fit a cubic function to a linear model, we use three features, let f be a feature, we then use f, f^2 and f^3 as three independent features and set them in a linear model
h(x) = t0 + ft1 + f^2t2 + f^3t3.

If features are chosen like this, feature scaling becomes increasingly important
Eg: h(x) = t0 + ft1 + sqrt(f)t2

Suppose you want to predict a house's price as a function of its size. Your model is
h(x) = t0 + (size)t1 + sqrt(size)t2
Suppose size ranges from 1 to 1000 (feet^2). You will implement this by fitting a model
h(x) = t0 + t1x1 + t2x2
Finally, suppose you want to use feature scaling (without mean normalization)
Which of the following choices for x1 and x2 should you use? (Note: sqrt(1000) ~ 32)
Ans: x1 = size/1000, x2 = sqrt(size)/32

NORMAL EQUATION

A method to solve for theta analytically.
Let J(theta) = a(theta^2) + b(theta) + c
find minima (when theta belongs to real numbers)
When theta is a vector, we can use partial derivatives to implement a similar process
Construct a matrix X that contains all features
Construct a vector y that contains all 'labels'
The value of theta that minimizes the cost function will then be:
theta = (X^T * X)^-1 * X^T * y
theta = inverse(transpose(X)*X)*transpose(X)*y
Set A = X^T * X. Then find A^-1
Octave: pinv(X'*X)*X'*y
Imp: For this method, feature scaling isn't actually necessary
Advantages: Normal equation doesn't need to choose learning rate and doesn't need to iterate.
Disadvantages: Normal equation doesn't work well with large data. Slow if n is large.
Cost of inverting a matrix is approximately O(n^3)
Inverting a 100x100 matrix is fast enough.
1000x1000 is also good enough
At about n = 10000, consider switching to other methods or to gradient descent.
Normal equations don't work for logistic regression and other sophisticated learning algorithms.

Non invertibility: (happens very rarely) pinv command in octave will actually compute the value we want without throwing errors.
Common causes of non-invertibility:
1. Redundant features: Eg: x1 = size in feet^2, x2 = size in m^2
2. Too many features (eg: m <= n) may cause non-invertibility
	Delete some features or use regularization.

OCTAVE
Differences: 
2^6 can be used
!= corresponds to ~= in Octave
% writes a comment
xor(1, 0)
writing a semicolon after a variable declaration prevents printing of the value of the assigned variable
a = pi % assigns value 'pi' to a
printing can be done using the disp() command
disp(sprintf('2 decimals: %0.2f', a))
format long % causes numbers to be displayed to a lot more decimal places
format short % the default value
A = [1 2; 3 4; 5 6] % generates a matrix
v = 1:0.1:2 % equivalent to v = range(1, 2, 0.1), v will be a row vector and the upper limit is inclusive
ones(2, 3) % generates a 2 by 3 matrix with all ones
2*ones(2, 3) 5 generates a 2 by 3 matrix with all twos
eye(n) % generates n dimensional identity matrix
rand(1, 3) % generates a row vector with random floats (0 to 1) from uniform distribution
randn(1, 3) % generates a row vector with random floats from gaussian distribution
hist(w) % generates histogram for the data
hist(w, n) % generates a histogram with n bins for the data
help eye % brings help page for eye
size(A) % returns a 1 by 2 matrix with the dimensions of the matrix A
PS1('>> ') % changes default terminal carat from octave > to >>
size(A, 1) % returns the size of the first dimension
length(A) % returns the longer dimension. Applied usually to vectors
load featuresX.dat % loads the file
load('featuresX.dat') % does the same
who % shows what variables we have in the octave workspace or in the octave memory
whos % gives a detailed view
clear featuresX % clears memory
v = priceY(1:10) % returns first 10 elements
save hello.mat v; % saves the vector v to a file called hello.mat (in a binary format)
save hello.txt v -ascii; % saves the vector v to a file called hello.txt (in a human readable ASCII format)
clear % deletes all variables in the workspace
A(3, 2) % returns the element in the third row and second column of A
A(2,:) % returns all elements in the second row
A(:, 2) % returns all the elements in the second column
A([1 3], :) % returns all the elements in the first and third rows
A(:, 2) = [10; 11; 12] replaces the second column with the given new column vector
A = [A, [100, 101, 102]] % appends the given column vector to the right
A(:) % puts all elements of A into a single column vector (equivalent to np.ravel())
C = [A B] % horizontally concatenates the matrices A and B into another matrix C
C = [A;B] % vertically concatenates the matrix A and B into the new matrix C
A*B % multiplies two matrices A and B
A .* B % multiplies the elements of the two matrices elementwise
A .^ 2 % squares all the elements of the matrix A
1 ./ A % reciprocates all the elements of the matrix A
log(A) % elementwise logarithm of A
exp(A) % elementwise exponentiation of A
abs(A) % elementwise magnitude of A
A + ones(size(A)) % increments all the elements of A by one
A + 1 % does the exact same thing
A' % returns transpose of A
max(A) % returns the maximum value of vector A
[val, ind] = max(A) % returns the maximum value of the vector and its index
A < 3 % returns a boolean matrix after doing the required logical comparison
find(A < 3) % returns the indices of the elements that return true to the specified logical operation
magic(n) % returns a matrix of dimension n by n in which, all the diagonals, rows, columns sum up to the same thing
[r, c] = find(A >= 7) % returns the row numbers and column numbers of the matrices that satisfy the given condition separately
sum(A) % adds up a vector A and columnwise adds up a matrix A
prod(A) % multiplies the elements of a vector A and columnwise multiplies a matrix A
floor(A), ceil(A) % do what they should
max(rand(3), rand(3)) % returns a matrix with the max elements from the two random three dimensional matrices
max(A, [], 1) % returns columnwise maximum of the matrix. 1 tells it to return the max along the first dimension (the row dimension). A 2 will make it return the max along the second dimension (the column dimension). A 3 will return the entire matrix for a 2-dimensional matrix
max(max(A)) % returns the largest element of the matrix A
max(A(:)) % (max(np.ravel(A))) returns the same
sum(sum(A.*eye(9))) % sums up the major diagonal of the matrix
B = A.*eye(9); sum(B(:)) % does the same
sum(sum(A.*flipud(eye(9)))) % sums the minor diagonal of the matrix
% flipud stands for flip-up-down and flips the matrix
plot(x_axis, y_axis) % plots data
% plotting a new plot deletes the old plot.
hold on % preserves the plotted plot and plots the next plot on top of it.
plot(t, y1, 'r') % changes color to red
xlabel('time'); ylabel('value') % labels axes (equivalent to matplotlib)
legend('sin', 'cos'); % adds a legend (equivalent to matplotlib)
title('Interesting graph') % adds title
print -dpng 'plot.png' % saves plot
close % causes the open figure to go away
figure(1); plot(t, y1) % plots first plot in a new figure
figure(2); plot(t, y2) % specifying a different figure number opens a new plot
subplot(1, 2, 1); % subdivides the plot into sections. The first two parameters divide the figure into a 1x2 grid. The third argument 'n' accesses the nth figure
subplot(1, 2, 1); plot(t, y1);
subplot(1, 2, 2); plot(t, y2); % plots two plots side by side
axis([0.5 1 -1 1]) % sets the xrange and the yrange on the plot
clf; % clears a figure
A = magic(5); imagesc(A) % plots a matrix's values as hues
imagesc(A), colorbar, colormap gray; % plots a grayscale colormatrix mapped to the values of the matirx A itself and displays a colorbar beside it as reference
commands can be comma chained to print the results of multiple commands
commands can be semi-colon chained to print the results of multiple commands
Let v = zeros(10, 1);
for i = 1: 10,
	v(i) = 2^i;
	end;
	% v now stores 2^i for i = 1 to 10

indices = 1:10;
for i = indices,
disp(i);
end;
% does the exact same thing
% break and continue can be used as usual

i = 1;
while i <= 5,
v(i) = 100;
i = i + 1;
end;
% does the same thing using a while loop
if, elseif, else % equivalent to if, elif, else
Functions:
function y = squareThisnumber(x)
y = x^2
% save this in a .m file to be called by octave from the wroking directory
addpath('C:\path\to\directory') % adds the specified path to the octave functions searchpath
Octave supports multiple return statements % check file sqcu.m to see how to define such a function
% to obtain the return values of such a function, use [a, b] = function(x) or something like that

VECTORIZATION
h(x) = sum(thetai*xi);
h(x) = theta' * x;

Unvectorized implementation (MATLAB/Octave):
prediction = 0.0;
for j = 1:n+1,
prediction = prediction + theta(j) * x(j)
end;
% IMPORTANT: Matlab and Octave are 1-indexed. Therefore, theta0 ends up having an index of 1 and so-on

Vectorized implementation (MATLAB/Octave):
prediction = theta' * x;

Unvectorized implementation in C++
double prediction = 0.0;
for(int j = 0; j <=n; ++j) {
	prediction == theta[j] * x[j];
}

Vectorized implementation in C++:
// Using a library
double prediction = theta.transpose() * x;
// here * is a C++ overloaded operator

Vectorization optimizes algorithms