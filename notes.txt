GRADIENT DESCENT

Time complexity: O(kn^2)
a = a - lr * delJ/dela
Always update terms simultaneously
Correct:
temp0 = a - lr*delJ/dela
temp1 = b - lr*delJ/delb
a = temp0
b = temp1

Incorrect:
temp0 = a - lr*delJ/dela
a = temp0
temp1 = b - lr*delJ.delb
b = temp1

The cost function for linear regression will always be a convex function. thus gradient descent will always converge to the local optimum
Batch gradient descent: each step uses all the training examples
Normal equations method: A method from linear algebra that directly finds the optimum, without needing to iteratively converge.
But, it was found that gradient descent was better for large datasets.

Wrong answer:
Linear regression with one variable:
Suppose that for some linear regression problem (say, predicting housing prices as in the lecture), we have some training set, and for our training set we managed to find some (theta)0, (theta)1 such that J(theta0, theta1) = 0. Which of the statements below must be true? (Multiple correct)

LINEAR ALGEBRA

vector: an n x 1 matrix
1-indexed vector, 0-indexed vector

Matrix without an inverse is a singular or degenerate matrix

MULTIVARIATE LINEAR REGRESSION

x^(i) = input (features) of ith training example
Single variable hypotheses: h(x) = t0 + xt1

Multivariable hypotheses: h(x) = t0 + x,t, + x2t2 + x3t3 + x4t4
let x0 = 1 to facilitate vectorization of the hypothesis function

X = column_vector([x0, x1, x2, x3 ... xn])
t = column_vector([t0, t1, t2, t3 ... tn])

h(x) = innerProduct((theta_transpose), x)
h(x) = x(theta)^T
General gradient descent
theta0 := theta_featureNumber - lr*(1/m) SUM(h(x^(houseNumber) - y^(houseNumber)))x_featureNumber^(houseNumber)

We can speed up gradient descent by having each of our input values in roughly the same range.
Gradient descent can work more quickly if the features are normalized or scaled
If the ellipse is thin, the gradient descent can take time and oscillate or meander about before converging
A scaled ellipse will be optimized earlier. GD can converge much faster
Get every feature to a [-1, 1] range
[0, 3] is acceptable; values in the vicinity are acceptable

Apply normalization too all features except x0
x(i) -> x(i) - mu(i)
i.e: replace x with (x - mean)/total

More general rule: 
replace x1 with (x1 - mean)/s1
where s1 = range(max - min)
or s1 = stdDeviation
(Quizzes in the course use range, programming assignments use standard deviation)

Feature scaling doesn't have to be too exact for gradient descent to work reasonably well

EG: Suppose you are using a learning algorithm to estimate the price of houses in a city. You want one of your features xi to capture the age of the house. In your training set, all of your houses have an age between 30 and 50 years, with an average of 38 years. Which of the following would you use as features assuming you use feature scaling and normalization?
Ans: x(i) = (age_of_house - 38) / 20

LEARNING RATE

Debugging gradient descent: Plot J(theta) vs no. of iterations. J(theta) should decrease after every iteration.
Example automatic convergence test: Declare convergence if J(theta) decreases by less than 10e-3 in one iteration.
If J(theta) is increasing, learning rate is probably too big.
For sufficiently small learning rate, J(theta) should decrease on every iteration.
Try learning rates as 0.0001, (0.0003), 0.001, (0.003), 0.01, (0.03), 0.1, (0.3), 1

POLYNOMIAL REGRESSION

We can create new features by ourselves
h(x) = t0 + xt1 + (x^2)t2
But quadratic models don't make practical sense as they have either a maxima or a minima, which might not correspond to probable values in the dataset.
Cubics might be better.
To fit a cubic function to a linear model, we use three features, let f be a feature, we then use f, f^2 and f^3 as three independent features and set them in a linear model
h(x) = t0 + ft1 + f^2t2 + f^3t3.

If features are chosen like this, feature scaling becomes increasingly important
Eg: h(x) = t0 + ft1 + sqrt(f)t2

Suppose you want to predict a house's price as a function of its size. Your model is
h(x) = t0 + (size)t1 + sqrt(size)t2
Suppose size ranges from 1 to 1000 (feet^2). You will implement this by fitting a model
h(x) = t0 + t1x1 + t2x2
Finally, suppose you want to use feature scaling (without mean normalization)
Which of the following choices for x1 and x2 should you use? (Note: sqrt(1000) ~ 32)
Ans: x1 = size/1000, x2 = sqrt(size)/32

NORMAL EQUATION

A method to solve for theta analytically.
Let J(theta) = a(theta^2) + b(theta) + c
find minima (when theta belongs to real numbers)
When theta is a vector, we can use partial derivatives to implement a similar process
Construct a matrix X that contains all features
Construct a vector y that contains all 'labels'
The value of theta that minimizes the cost function will then be:
theta = (X^T * X)^-1 * X^T * y
theta = inverse(transpose(X)*X)*transpose(X)*y
Set A = X^T * X. Then find A^-1
Octave: pinv(X'*X)*X'*y
Imp: For this method, feature scaling isn't actually necessary
Advantages: Normal equation doesn't need to choose learning rate and doesn't need to iterate.
Disadvantages: Normal equation doesn't work well with large data. Slow if n is large.
Cost of inverting a matrix is approximately O(n^3)
Inverting a 100x100 matrix is fast enough.
1000x1000 is also good enough
At about n = 10000, consider switching to other methods or to gradient descent.
Normal equations don't work for logistic regression and other sophisticated learning algorithms.

Non invertibility: (happens very rarely) pinv command in octave will actually compute the value we want without throwing errors.
Common causes of non-invertibility:
1. Redundant features: Eg: x1 = size in feet^2, x2 = size in m^2
2. Too many features (eg: m <= n) may cause non-invertibility
	Delete some features or use regularization.

OCTAVE
Differences: 
2^6 can be used
!= corresponds to ~= in Octave
% writes a comment
xor(1, 0)
writing a semicolon after a variable declaration prevents printing of the value of the assigned variable
a = pi % assigns value 'pi' to a
printing can be done using the disp() command
disp(sprintf('2 decimals: %0.2f', a))
format long % causes numbers to be displayed to a lot more decimal places
format short % the default value
A = [1 2; 3 4; 5 6] % generates a matrix
v = 1:0.1:2 % equivalent to v = range(1, 2, 0.1), v will be a row vector and the upper limit is inclusive
ones(2, 3) % generates a 2 by 3 matrix with all ones
2*ones(2, 3) 5 generates a 2 by 3 matrix with all twos
eye(n) % generates n dimensional identity matrix
rand(1, 3) % generates a row vector with random floats (0 to 1) from uniform distribution
randn(1, 3) % generates a row vector with random floats from gaussian distribution
hist(w) % generates histogram for the data
hist(w, n) % generates a histogram with n bins for the data
help eye % brings help page for eye
size(A) % returns a 1 by 2 matrix with the dimensions of the matrix A
PS1('>> ') % changes default terminal carat from octave > to >>
size(A, 1) % returns the size of the first dimension
length(A) % returns the longer dimension. Applied usually to vectors
load featuresX.dat % loads the file
load('featuresX.dat') % does the same
who % shows what variables we have in the octave workspace or in the octave memory
whos % gives a detailed view
clear featuresX % clears memory
v = priceY(1:10) % returns first 10 elements
save hello.mat v; % saves the vector v to a file called hello.mat (in a binary format)
save hello.txt v -ascii; % saves the vector v to a file called hello.txt (in a human readable ASCII format)
clear % deletes all variables in the workspace
A(3, 2) % returns the element in the third row and second column of A
A(2,:) % returns all elements in the second row
A(:, 2) % returns all the elements in the second column
A([1 3], :) % returns all the elements in the first and third rows
A(:, 2) = [10; 11; 12] replaces the second column with the given new column vector
A = [A, [100, 101, 102]] % appends the given column vector to the right
A(:) % puts all elements of A into a single column vector (equivalent to np.ravel())
C = [A B] % horizontally concatenates the matrices A and B into another matrix C
C = [A;B] % vertically concatenates the matrix A and B into the new matrix C
A*B % multiplies two matrices A and B
A .* B % multiplies the elements of the two matrices elementwise
A .^ 2 % squares all the elements of the matrix A
1 ./ A % reciprocates all the elements of the matrix A
log(A) % elementwise logarithm of A
exp(A) % elementwise exponentiation of A
abs(A) % elementwise magnitude of A
A + ones(size(A)) % increments all the elements of A by one
A + 1 % does the exact same thing
A' % returns transpose of A
max(A) % returns the maximum value of vector A
[val, ind] = max(A) % returns the maximum value of the vector and its index
A < 3 % returns a boolean matrix after doing the required logical comparison
find(A < 3) % returns the indices of the elements that return true to the specified logical operation
magic(n) % returns a matrix of dimension n by n in which, all the diagonals, rows, columns sum up to the same thing
[r, c] = find(A >= 7) % returns the row numbers and column numbers of the matrices that satisfy the given condition separately
sum(A) % adds up a vector A and columnwise adds up a matrix A
prod(A) % multiplies the elements of a vector A and columnwise multiplies a matrix A
floor(A), ceil(A) % do what they should
max(rand(3), rand(3)) % returns a matrix with the max elements from the two random three dimensional matrices
max(A, [], 1) % returns columnwise maximum of the matrix. 1 tells it to return the max along the first dimension (the row dimension). A 2 will make it return the max along the second dimension (the column dimension). A 3 will return the entire matrix for a 2-dimensional matrix
max(max(A)) % returns the largest element of the matrix A
max(A(:)) % (max(np.ravel(A))) returns the same
sum(sum(A.*eye(9))) % sums up the major diagonal of the matrix
B = A.*eye(9); sum(B(:)) % does the same
sum(sum(A.*flipud(eye(9)))) % sums the minor diagonal of the matrix
% flipud stands for flip-up-down and flips the matrix
plot(x_axis, y_axis) % plots data
% plotting a new plot deletes the old plot.
hold on % preserves the plotted plot and plots the next plot on top of it.
plot(t, y1, 'r') % changes color to red
xlabel('time'); ylabel('value') % labels axes (equivalent to matplotlib)
legend('sin', 'cos'); % adds a legend (equivalent to matplotlib)
title('Interesting graph') % adds title
print -dpng 'plot.png' % saves plot
close % causes the open figure to go away
figure(1); plot(t, y1) % plots first plot in a new figure
figure(2); plot(t, y2) % specifying a different figure number opens a new plot
subplot(1, 2, 1); % subdivides the plot into sections. The first two parameters divide the figure into a 1x2 grid. The third argument 'n' accesses the nth figure
subplot(1, 2, 1); plot(t, y1);
subplot(1, 2, 2); plot(t, y2); % plots two plots side by side
axis([0.5 1 -1 1]) % sets the xrange and the yrange on the plot
clf; % clears a figure
A = magic(5); imagesc(A) % plots a matrix's values as hues
imagesc(A), colorbar, colormap gray; % plots a grayscale colormatrix mapped to the values of the matirx A itself and displays a colorbar beside it as reference
commands can be comma chained to print the results of multiple commands
commands can be semi-colon chained to print the results of multiple commands
Let v = zeros(10, 1);
for i = 1: 10,
	v(i) = 2^i;
	end;
	% v now stores 2^i for i = 1 to 10

indices = 1:10;
for i = indices,
disp(i);
end;
% does the exact same thing
% break and continue can be used as usual

i = 1;
while i <= 5,
v(i) = 100;
i = i + 1;
end;
% does the same thing using a while loop
if, elseif, else % equivalent to if, elif, else
Functions:
function y = squareThisnumber(x)
y = x^2
% save this in a .m file to be called by octave from the wroking directory
addpath('C:\path\to\directory') % adds the specified path to the octave functions searchpath
Octave supports multiple return statements % check file sqcu.m to see how to define such a function
% to obtain the return values of such a function, use [a, b] = function(x) or something like that

VECTORIZATION
h(x) = sum(thetai*xi);
h(x) = theta' * x;

Unvectorized implementation (MATLAB/Octave):
prediction = 0.0;
for j = 1:n+1,
prediction = prediction + theta(j) * x(j)
end;
% IMPORTANT: Matlab and Octave are 1-indexed. Therefore, theta0 ends up having an index of 1 and so-on

Vectorized implementation (MATLAB/Octave):
prediction = theta' * x;

Unvectorized implementation in C++
double prediction = 0.0;
for(int j = 0; j <=n; ++j) {
	prediction == theta[j] * x[j];
}

Vectorized implementation in C++:
// Using a library
double prediction = theta.transpose() * x;
// here * is a C++ overloaded operator

Vectorization optimizes algorithms

Q:
v = zeros(10, 1)
for i = 1:10,
	for j = 1:10,
		v(i) = v(i) + A(i, j) * x(j);
	end;
end;

To find product Ax, how would you vectorize this code without any for loops?
Hint: Dimension of the answer vector matters. 10x1 ~= 1x10

LOGISTIC REGRESSION

For classification problems.
Binary classification problems
Hypotheses representation:
We need 0 <= h(x) <= 1
h(x) = g(theta' * x)
where g is the sigmoid function or the logistic function
g(z) = 1 / (1 + e^-z)
g(z) outputs a value between 0 and 1
It has asymptotes at y = 1 and y = 0
Thus, h(x) must also be between 0 and 1
When h(x) outputs a number, we will treat that number as the estimated probability that y = 1 on input x
If h(x) = 0.7,
We will predict that there is a 70% chance of the output being 1
h(x) = P(y = 1|x ; theta) % read as 'probability that y = 1, given x, parameterized by theta'
y must be 0 or 1
P(y = 0|x ; theta) + P(y = 1|x ; theta) = 1
P(y = 0|x ; theta) = 1 - P(y = 1|x ; theta)
To normalize outputs, 
predict 'y = 1' if h(x) >= 0.5
predict 'y = 0' if h(x) < 0.5
g(z) >= 0.5 whenever z >= 0
h(x) = g(theta' * x)
h(x) >= 0.5 whenever (theta' * x) >= 0
h(x) < 0.5 whenever (theta' * x) < 0
Let h(x) = g(t0 + t1x1 + t2x2)
Let t = [-3;1;1]
Predict 'y = 1' if -3 + x1 + x2 >= 0, for some x1, x2 where x1 + x2 >= 3
Plotting x1 + x2 = 3 on a graph where the axes are x1 and x2, gives the line known as the decision boundary
x1 + x2 = 3 corresponds to h(x) = 0.5
We don't need to plot a training set in order to plot the decision boundary
Non-linear decision boundaries
We can add nonlinear features, eg x1, x2, x1^2, x2^2, etc to fit our model
Eg: if h(x) = g(t0 + t1x1 + t2x2 + t3x1^2 + t4x2^2)
if t0 = -1, t1 = 0, t2 = 0, t3 = 1, t4 = 1, we get a circle with radius 1
theta = [-1;0;0;1;1]
Therefore, predict 'y = 1' if -1 + x1^2 + x2^2 >= 0
Decision boundary is a property of the hypotheses, not the training set.
Once we have the parameter theta, that defines the decision boundary, not the training set
Using more complicated functions as the hypotheses, we can come up with complex-shaped decision boundaries
In retrospect:
h(x) >= 0.5 -> y = 1
h(x) < 0.5 -> y = 0
g(z) >= 0.5, when z >= 0
Thus, when theta' * x >= 0 => y = 1
when theta' * x < 0 => y = 0
Cost function:
m examples, x belongs to [x0, x1, x2...] where x0 is always 1. For every x, the label y is either 0 or 1
cost(h(x), y) = 1/2(h(x) - y)^2
In logistic regression, the cost function is non-convex
Using a squared cost function, the non-linear sigmoid term causes many local optima in the cost function. We would like te cost function to be convex. We might come up with a different cost function that is convex, so that we can apply gradient descent
We redefine the cost function as follows
Cost(h(x), y) = { -log(h(x)),  		if y = 1;
				{ -log(1 - h(x)), 	if y = 0;
J = 1/m(sum(Cost(i)))
This function works very well in this case as cost = 0 if y = 1, h(x) = 1
But as h(x) -> 0, Cost -> infinity
Captures the intuition that if h(x) = 0, (predict P(y = 1|x ; theta) = 0), but y = 1, we'll penalize the learning algorithm by a very large cost
For y = 0, the cost function looks flipped (a mirror image along the y axis) between the x values of 0 and 1.
Works on a similar intuition as explained above
Simplified cost function: 
Cost(h(x), y) = -ylog(h(x)) - (1 - y)log(1 - h(x))
Cost(h(x), y) = -log(1 - y + (-1)^(y+1)*h(x))
Thus, J = -1/m[sum(ylog(h(x)) + (1 - y)log(1 - h(x)))]
Vectorized: J(theta) = (1/m)*(-y'*log(h) - (1-y)'*log(1 - h))
this cost function is used, because it can be analysed in great detail by statistics using the principle of likelihood estimation. And this function is convex and used by most people for logistic regression.
To fit parameters, min J(theta) wrt theta
Gradient descent:
Repeat {
	thetaj = thetaj - learning_rate * sum((h(x) - y)*xj(i))
}
algorithm looks identical to linear regression. (h(x) is as different as can be)
Apply same methods to monitor gradient descent as in linear regression
Q: Vectorized implementation of gradient descent for logistic regression
Ans: theta = theta - learning_rate*(1/m)*sum[(h(x) - y) * x]
Vectorized implementation: theta = theta - (learning_rate/m)*X'*(g(X*theta) - y)
The idea of feature scaling applies to gradient descent for logistic regression too
To make logistic regression run quicker and scale much better to larger machine learning problems.
Optimization algorithms:
Gradient descent, Conjugate gradient, BFGS, L-BFGS; These have a clever inner loop called a line-search algorithm that automatically tries different learning rates
Advantages of algorithms other tan gradient descent:
no need to manually pick the learning rate alpha,
These are often faster than gradient descent
Disadvantages: more complex (do not implement these yourself)
Eg for costFunction
function [jVal, gradient] = costFunction(theta)
jVal = (theta(1) - 5)^2 + (theta(2) - 5)^2;
gradient = zeros(2, 1);
gradient(1) = 2*(theta(1) - 5);
gradient(2) = 2*(theta(2) - 5);
% Advanced optimization function fminunc() (function minimization unconstrained)
% options is a data structure that stores the options
% sets gradientObjective parameter to 'on'
options = optimset('GradObj', 'on', 'MaxIter', 100);
initialTheta = zeros(2, 1);
[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options)
% Gradient descent on steroids
% the exitFlag helps to verify whether or not the algorithm thinks it has converged
% parameter vector initialTheta has to be at least 2 dimensional, or fminunc will give funny results

Generalized costFunction:
function [jval, gradient] = costFunction(theta)
jVal = [code to compute J(theta)]
gradient(1) = [code to compute the partial derivative wrt theta0]
gradient(2) = [code to compute the partial derivative wrt theta1]
...
gradient(n+1) = [code to compute the partial derivative wrt thetan]

MULTI CLASS CLASIFICATION
OVR type, where y can take a small number of discrete values
Train three classifiers simultaneously and output the value of the classifier that was the most confident. If we have k classes, we have to train k classifiers
Q: Suppose you have the following training set, and fit a logistic regression classifier 
Which of the following are true? Check all that apply.
Hint: the one that says J(theta) will be greater when the features are combined (x1x2, x2x3, x2x1^2, etc) is wrong

THE PROBLEM OF OVERFITTING
Underfit ~ high bias
Overfit ~ high variance
Overfitting: If we have too many features, the learned hypothesis may fit the training set very well, (J(theta) ~ 0), but fail to generalize to new examples (predict prices on new examples)
Addressing overfitting:
Reduce number of features
-Manually select which features to keep
-Model selsction algorithm
Regularization
-Keep all the features, but reduce the magnitude/values of parameters theta(j)
-Works well when we have a lot of features, each of which contributes a bit to predicting y. (a lot of slightly useful features)

REGULARIZATION
Suppose we penalize and make the higher degree terms really small
consider J(theta) = J(theta) + 1000theta_3^2 + 1000theta_4^2
If we have small values of parameters, we will have a much simpler (and smoother) hypothesis which will be less prone to overfitting
Therefore, we add an extra regularization term at the end to shrink our parameters, so that
J(theta) = 1/2m[sum(h(theta) - y)^2 + lambda*sum(theta)]
where the coefficient of lambda is summed for theta = 1 to n. We don't want to penalize theta0. (It makes very little difference, but it is the convention)
lambda : regularization parameter
A very high lambda causes underfitting, as most parameters become almost zero

REGULARIZED LINEAR REGRESSION
Theta is updated separately
Gradient descent: thetaj = thetaj(1 - alpha*lambda/m) - alpha/m(sum(h(x) - y) * x)
Normal form: theta = (X'X + lambda[M])*X'y
where M is a (n+1) x (n+1) matrix where the first element is zero, all the (major) diagonal elements are 1 and everything else is 0
Non-invertibility
Suppose m <= n
(#examples), (#features)
in this case,
theta = pinv(X'X)*X'y, the pinv(X'X) term will be non-invertible, singular or degenerate
% pinv gives a correct output anyway, but it won't give a very good hypothesis
Regularization takes care of this as well
Concretely, if lambda > 0, the matrix (X'X + lambda.[M]) is invertible

REGULARIZED LOGISTIC REGRESSION
J(theta) = -[(1/m)*sum(y.log(h(x) + (1 - y).log(1 - h(x))))] + lambda/2m.sum(theta(j))^2
The new cost function looks cosmetically the same as the one for the linear regression, but here, h(x) uses the sigmoid function

ADVANCED OPTIMIZATION FOR LOGISTIC REGRESSION
function [jVal, gradient] = costFunction(theta)
jval = [code to compute J(theta)];
gradient(1) = [code to compute partial derivative of J(theta) wrt theta0];
gradient(2) = [code to compute partial derivative of J(theta) wrt theta1];
...
gradient(n+1) = [code to compute partial derivative of J(theta) wrt thetan];
Q: You are training a classification model with logistic regression. Which of the following statements are true? Check all that apply
A: Adding a new feature to the model always results in equal or better performance on the training set
B: Adding many new features to the model helps prevent overfitting on the training set
C: Introducing regularization to the model always results in equal or better performance on the training set
D: Introducing regularization to the model always results in equal or better performance on examples not in the training set
A, D is wrong
only C is also wrong

NEURAL NETWORKS
a_i^(j) = activation of unit i in layer j
theta^(j) = matrix of weights controlling function mapping from layer j to layer j + 1
If network has s_j units in layer j, s_(j+1) units in layer j + 1, then theta^(j) will be of dimension s_(j+1), then theta^(j) will be of dimension s_(j+1) x (s_j + 1)
z_1^(2) = theta_10^(1)*x_0 + ...
h(x) = a^(3) = g(z^(3))
z^(2) = theta^(1)*a^(1); a^(2) = g(z^(2))
a^(j) = g(z^(j))
For predicting AND values, consider h(x) = g(-30 + 20x1 + 20x2)
x1 		x2 		h(x)
0 		0 		g(-30) ~ 0
0 		1 		g(-10) ~ 0
1 		0		g(-10) ~ 0
1		1		g(+10) ~ 1
h(x) ~ X1 AND X2
For predicting NOT values, consider h(x) = g(10 - 20x1)
x1 		h(x)
0		g(+10) ~ 1
1		g(-10) ~ 0
The general idea of negation is to have large negative weights for the variable
(NOT x1) AND (NOT x2) can be approximated by h(x) = 10 - 20x1 - 20x2
Multi-class classification in neural networks is an example of one-vs-all classification
Output will be a one-hot vector with dimensions equal to the number of classes
Example octave code:
% theta1 is Theta with superscript "(1)" from lecture
% ie, the matrix of parameters for the mapping from layer1 (input) to layer2
% Theta1 has size 3x3
% Assume 'sigmoid' is a built-in function to compute 1 / (1 + exp(-z))

a2 = zeros(3, 1);
for i = 1:3,
	for j = 1:3,
		a2(i) = a2(i) + x(j) * Theta1(i, j);
	end;
	a2(i) = sigmoid(a2(i))
end;

NEURAL NETWORK FOR CLASSIFICATION
L = total number of layers in network
s_l = total number of units (not counting bias unit) in layer l
s_L = total number of units in layer L (number of output nodes)
Binary classification: y = 0 or 1 (1 output unit) s_L = K = 1
Multi-class classification: K output units of one-hot vectors s_L = K
Cost function of a neural network will be a generalization of logistic regression
h(x) will now be a k-dimensional vector. The label vector y will also be k-dimensional per example
Q: Suppose we try to minimize J(theta) as as function of theta, using one of the advanced optimization methods (fminunc, conjugate-gradient, BFGS, L-BFGS, etc.). What do we need to supply code to compute (as a function of theta)?
Ans: J(theta) and the partial derivative terms del/del(theta_ij) for every i, j, l
Backpropagation:
Consider a neural network with these numbers of nodes in its layers: 3->5->5->4
For forward propagation, we calculate the following:
a^(1) = x
z^(2) = theta^(1)a^(1)
a^(2) = g(z^(2)) (add(a_0^(2)))
z^(3) = theta^(2)a^(2)
a^(3) = g(z^(3)) (add(a_0^(3)))
z^(4) = theta^(3)a^(3)
a^(4) = h(x) = g(z^(4))
del_j^(l) = "error" of node j in layer l
For each output unit (layer L = 4)
del_j^(4) = a_j^(4) - y_j
where a_j^(4) = h(x)_j
Vectorized: del^(4) = a^(4) - y
del^(3) = (theta^(3))'del^(4).*g'(z^(3))
del^(2) = (theta^(2))'del^(3).*g'(z^(2))
g'(z^(3)) = a^(3).*(1 - a^(3))
No del^(1) term. No error associated with the features
If regularization is ignored, 
del/(del(theta))(J(theta)) = a_j^(l)*d_i^(l+1)
Suppose training set has m examples
Set deltas = 0
% deltas will be used as accumulators, which we will add things to as we move through back-propagation
for i = 1 : m,
	set a^(1) = x^(i)
	Perform forward propagation to compute a^(l) for l = 2, 3 ... L
	Using y^(i), compute del^(L) = a^(L) - y^(i)
	Compute del^(L - 1), del^(L - 2) ... del^(2)
	delta_ij^(l) = delta_ij^(l) + a_j^(l)*del_j^(l + 1)
D_ij^(l) = 1/m*delta_ij^(l) + lambda*theta_ij^(l) if j != 0
D_ij^(l) = 1/m*delta_ij^(l) if j = 0
del(J(theta))/del(theta_ij^(l)) = D_ij^(l)
del_j^(l) = partial derivative of cost(i)
In the cost function of a neural network, the double sum simply adds up the logistic regression costs calculated for each cell in the output layer, the triple sum simply adds up the squares of all the individual thetas in the entire network
Implementation of backpropagation:
We need to unroll our matrices into vectors in order to use one of the more advanced optimization routines
These routines assume that thetas are vetors, also assumes that the cost function returns vectors too
In ANNs, The costs are thetas, the gradients are Ds
In Octave/MATLAB, unrolling can be done as: thetaVec = [Theta1(:); Theta2(:), Theta3(:)];
DVec = [D1(:); D2(:); D3(:)];
To get back our matrices after optimization, we use the following commands:
Theta1 = reshape(thetaVec(1:110), 10, 11);
Theta2 = reshape(thetaVec(111:220), 10, 11);
Theta3 = reshape(thetaVec(221:231), 1, 11);
Learning algorithm:
Have initial parameters: Theta1, Theta2, Theta3,
Unroll to get initialTheta to pass to
fminunc (@costFunction, initialTheta, options)
...
function [jVal, gradientVec] = costFunction(thetaVec)
	From thetaVec, get Theta1, Theta2, Theta3 (reshape)
	Use forward propagation/backward propagation to compute D^(1), D^(2), D^(3) and J(theta).
	Unroll D^(1), D^(2), D^(3) to get gradientVec.
Gradient Checking (Numerical estimation of gradients)
A method to reduce errors and prevent subtle untraceable schrodinbugs in implementations of back-propagation reasonably complex models. It pretty much eliminates bugs.
Two-sided numerical gradient is slightly more accurate than one-sided numerical gradient for a bit larger values of epsilon.
Implementation:
(J(theta + EPSILON) - J(theta - EPSILON))/(2*EPSILON)
theta belongs to R^n (eg, theta is unrolled version of theta(1), theta(2), theta(3))
theta = [theta1, theta2, theta3 ... thetan]
delJ(theta)/del(theta) ~ (J(theta1 + epsilon, theta2 ... thetan) - J(theta1 - epsilon, theta2, theta3 ... thetan))/2*epsilon
TO numerically compute the derivative, we implement the following in octave:
EPSILON = 1e-4;
for i = 1:n,
	thetaPlus = theta; % the unrolled version of theta
	thetaPlus(i) = thetaPlus(i) + EPSILON;
	thetaMinus = theta;
	thetaMinus(i) = thetaMinus(i) - EPSILON;
	gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*EPSILON);
end;
Check that gradApprox ~ DVec
Implementation note:
Implement backprop to compute DVec (unrolled Ds)
Implement numerical gradient checking to compute gradApprox
Make sure both of these give similar values
Turn off gradient checking. Using backprop code for learning
Important:
Be sure to disable your gradient checking code before training your classifier. If you run numerical gradient computation on every classifier of gradient descent (or in the inner loop of the costFunction()) your code will be very slow.
Random Initialization:
Initializing all thetas to zero does not work when we are training a neural network.
After each update, parameters corresponding to inputs going into each of two hidden units are identical.
Therefore, the neural network cannot compute very interesting functions
Random initialization: Symmetry breaking
Initialize each theta to a random value in [-epsilon, epsilon] % different epsilon than the one in gradient checking
Eg:
Theta1 = rand(10, 11) * (2 * INIT_EPSILON) - INIT_EPSILON; % because of this piece of code, we get thetas between - and + epsilon
Theta2 = rand(1, 11) * (2 * INIT_EPSILON) - INIT_EPSILON;
Putting it together:
No of input units: dimension of features x
No of output units: number of classes
Reasonable default; 1 hidden layer, or if > 1 hidden layer, have same no. hidden units in every layer. Very often having more hidden units is a good thing.
Training a neural network;
1. Rnadomly initialize the weights
2. Implement forward propagation to get h(x) for any x^(i)
3. Implement code to compute cost function J(theta)
4. Implement backprop to compute partial derivatives
for i = 1:m, (for can be gotten rid of using advanced vectorization)
	Perform forward propagation and backpropagation using example (x^(i), y^(i))
	(Get activations a^(l) and delta terms del^(l) for l = 2 ... L)
5. Use gradient checking to compare partial derivatives computed using backpropagation vs using numerical estimate of gradient of J(theta)
6. Use gradient descent or one of the more advanced optimization algorithms with backpropagation to try to minimize J(theta) as a function of parameters theta.
For backpropagation, the cost function J(theta) is not convex. It is theoretically possible to get stuck in local optima. In practice, this doesn't turn out to be a very big problem
Q: Suppose you are using gradient descent together with backpropagation to try to minimize J(theta) as a function of theta. Which of the following would be a useful step for verifying that the learning algorithm is running correctly?
Ans: Plot J(theta) as a function of number of iterations and make sure it is decreasing (or at least non-increasing) with every iteration.